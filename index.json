[{"content":"kolla-helm 包含一些列 helm chart，基于这些 chart 我们可以很容易的在 k8s 平台上部署 openstack， 在 openstack 社区有一个 openstack-helm 项目提供了类似的功能，但是由于 openstack-helm 设计过于复杂，项目活跃度底，不兼容最新 helm，所以我们打算提供一套新的 chart。 我们为什么叫 kolla-helm 呢？顾名思义，我们想要借助 kolla 容器来完成新版 chart 的编写。\n借助这些 chart 你可以有多种方式来轻松的部署 openstack\n通过命令行部署 你可以通过 helm 命令部署 openstack。\n图形化部署 你还可以通过 kubeapp 提供的 web UI 轻松的部署 openstack。\n","description":"","tags":null,"title":"kolla-helm","uri":"/documents/kolla-helm/"},{"content":"","description":"","tags":null,"title":"准备工作","uri":"/documents/kolla-helm/preparation/"},{"content":"kubeadm 是 k8s 官方提供的一个部署管理 k8s 集群的工具，但是 kubeadm 使用到的很多资源的下载地址都在国外，如果按照官方文档操作很容易因为网络原因失败。这里基于 ubuntu 20.04 展示如何让 kubeadm 使用国内的资源部署 k8s 集群。\n下面所有命令都是使用 root 用户执行的\n安装 docker 参照 docker 官方安装文档\n$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg $ echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null $ apt-get update $ apt-get install docker-ce docker-ce-cli containerd.io 通过阿里源安装：\n$ curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg $ echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://mirrors.aliyun.com/docker-ce/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null $ apt-get update $ apt-get install docker-ce docker-ce-cli containerd.io 安装 kubeadm 官方文档使用的 https://apt.kubernetes.io/ apt 源在国内被屏蔽了，所以我们需要找一个国内的镜像源，这里我们以阿里源为例\napt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat \u003c\u003cEOF \u003e/etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF apt update 在安装之前可以通过下面命令我们可以安装的 kubeadm 的版本\napt-cache madison kubeadm 根据情况选择适合的版本安装\napt-get install kubectl=1.23.7-00 kubelet=1.23.7-00 kubeadm=1.23.7-00 注意：这里选择的版本决定了最终安装的 k8s 的版本，从 1.24 开始 k8s 从代码中彻底移除了 dockershim，所以我们这里选择用 k8s 1.23.7，如果想在 k8s 1.24 及以后使用 docker 作为 cri，则需要安装一个外部的 dockershim，mirantis 的 cri-dockerd 是一个不错的选择。\n节点准备 关闭 swap 分区 ( 貌似最新的 k8s 版本不在要求这一步 )\nsudo swapoff -a # 暂时关闭，永久关闭可以上网查询 初始化环境 创建一个 yaml 文件，我们命名为 kubeadm-init-config.yaml，填入以下内容\n---apiVersion:kubeadm.k8s.io/v1beta3kind:InitConfigurationbootstrapTokens:- token:abcdef.0123456789abcdefttl:24h0m0slocalAPIEndpoint:advertiseAddress:172.18.30.127bindPort:6443nodeRegistration:criSocket:/var/run/dockershim.sockimagePullPolicy:IfNotPresenttaints:[]---apiVersion:kubeadm.k8s.io/v1beta3kind:ClusterConfigurationapiServer:timeoutForControlPlane:4m0scertificatesDir:/etc/kubernetes/pkiclusterName:kubernetescontrollerManager:{}dns:{}etcd:local:dataDir:/var/lib/etcdimageRepository:registry.aliyuncs.com/google_containerskubernetesVersion:1.23.0networking:dnsDomain:cluster.localserviceSubnet:10.96.0.0/12podSubnet:10.244.0.0/16scheduler:{}---apiVersion:kubelet.config.k8s.io/v1beta1kind:KubeletConfigurationfailSwapOn:falseaddress:0.0.0.0enableServer:truecgroupDriver:cgroupfs---apiVersion:kubeproxy.config.k8s.io/v1alpha1kind:KubeProxyConfigurationmode:ipvsipvs:strictARP:true这里有几个参数要特别注意一下：\n advertiseAddress：这个要改为当前主机的管理网 IP 地址 imageRepository：这是一个关键的配置，从国内源下载相关镜像 podSubnet: 这个是下面部署的 flannel cni 插件要求的一个参数，需要和 flannel 网络配置 net-conf.json 中的 Network 保持一致 cgroupDriver: 这个需要和 docker 的 Cgroup Driver 保持一致，可以通过命令 docker info|grep \"Cgroup Driver\" 查看  然后执行：\nsudo kubeadm init --config kubeadm-init-config.yaml 如果你不想关闭 swap 分区，使用下面命令初始化\nsudo kubeadm init --config kubeadm-init-config.yaml --ignore-preflight-errors Swap 等待一会儿，初始化成功后会获得如下输出：\nYour Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 172.18.30.127:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:159072d62927901183f5cda29fbb4e110ee8e354a350aad7774239e19c57169a 按照输出提示执行下面命令配置 kubeconfig 以便后面我们能正常使用 kubectl 命令\nmkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 安装网络插件 在安装网络插件之前我们查看一下 node 信息\n# kubectl get nodes NAME STATUS ROLES AGE VERSION yjf-kubeadm NotReady master 4h8m v1.19.16 发现 node 的状态为 NotReady\n通过下面命令安装 flannel 网络插件\n$ sudo kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml podsecuritypolicy.policy/psp.flannel.unprivileged created clusterrole.rbac.authorization.k8s.io/flannel created clusterrolebinding.rbac.authorization.k8s.io/flannel created serviceaccount/flannel created configmap/kube-flannel-cfg created daemonset.apps/kube-flannel-ds created 等一会儿，大概 5 分钟之后在查看 node 信息\n# kubectl get nodes NAME STATUS ROLES AGE VERSION kubeadm Ready master 4h20m v1.19.16 node status 已经变为 ready 了\n添加节点 在新的节点重新执行 安装 docker，安装 kubeadm，节点准备 三个步骤。\n然后执行\nkubeadm join 172.18.30.127:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:a86f804bc33460936ce8c6a9bdde774190815fafcd5a093c0d53a8f7bfc72ad3 执行完成后在第一个节点查看 nodes\n# kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME kubeadm Ready master 42m v1.19.16 172.18.30.127 \u003cnone\u003e Ubuntu 20.04.3 LTS 5.11.0-34-generic docker://20.10.12 kubeadm2 Ready \u003cnone\u003e 4m43s v1.19.16 172.18.30.151 \u003cnone\u003e Ubuntu 20.04.3 LTS 5.11.0-34-generic docker://20.10.12 ","description":"","tags":null,"title":"在国内使用 kubeadm 部署 k8s","uri":"/documents/post/kubeadm/"},{"content":"","description":"","tags":null,"title":"WIKI","uri":"/documents/post/"},{"content":"","description":"","tags":null,"title":"通用中间件部署","uri":"/documents/kolla-helm/dependency/"},{"content":"","description":"","tags":null,"title":"部署 keystone","uri":"/documents/kolla-helm/keystone/"},{"content":"","description":"","tags":null,"title":"部署 cinder","uri":"/documents/kolla-helm/cinder/"},{"content":"","description":"","tags":null,"title":"Categories","uri":"/documents/categories/"},{"content":"kungze Kungze 是由一群从事云计算方面工作小伙伴组建的一个兴趣小组，我们的目的是想把我们的一些脑洞变成可用的项目并提供给大家使用，当前我们的工作主要聚焦在 kubernetes 和 openstack，当前的目标是打造一套 kubernetes 和 openstack 混合交付方案 (我们不想 提供一套 k8s on openstack 的方案，我相信这类方案市场上有很多了)，在我们的部署方案中 kubernetes 和 openstack 是平级的，结 合两者的长处打造一个稳定，高效，灵活且易于部署的云平台。\nK8S openstack 融合 k8s 能弥补 openstack 的哪些不足？\n 简化 openstack 的部署，我们第一阶段的主要工作就是打造一个通过 helm 轻松部署 openstack 的方案 弥补 openstack 应用市场和 DBaaS 的不足 丰富 openstack 日志和监控告警的手段  openstack 能给 k8s 带来哪些好处？\n 借助 openstack cinder 提供多种后端存储 借助 openstack neutron 提供丰富灵活的 CNI 插件 借助 openstack keystone 完善 k8s 租户体系  基于以上的想法我们打算提供一些列的工具。\nkolla-helm 仓库地址：https://github.com/kungze/kolla-helm。这个项目包含一些列 helm chart，基于这些 chart 我们可以很容易的在 k8s 平台上部署 openstack， 在 openstack 社区有一个 openstack-helm 项目提供了类似的功能，但是由于 openstack-helm 设计过于复杂，项目活跃度底，不兼容最新的 helm v3，所以我们打算提供一套新的 chart。 我们为什么叫 kolla-helm 呢？顾名思义，我们想要借助 kolla 容器来完成新版 chart 的编写。\ncinder-metal-csi 由于 k8s 官方提供的 cinder-csi-plugin 只适用于 k8s on openstack 的场景 (即：k8s 需 要运行在 openstack 虚机里面)。所以我们打算提供一套新的 cinder csi 插件，使运行在裸金属系统上的容器也能 很方便的使用 cinder 存储。这个项目我们还在规划中，目前还没有仓库地址。\n其他奇奇怪怪的项目 除了我们的主线任务：k8s 与 openstack 融合所需的项目。我们还有一些成员脑洞的一些项目，这些项目可能在某些特殊的场景有一些意想不到的用途。\nquic-tun 仓库地址：https://github.com/kungze/quic-tun。这个项目原理上和 kcptun 类似，都是把 TCP 数据包转为 UDP 数据包，借助 UDP 的特性加上一些优秀的重传算法优化流量在公网上的传输。但是这两个项目的目的有很大区别。quic-tun 是基于 google 的 quic 协议实现的（quic 是基于 UDP 的）。其不光有优化网络传输的功能，还有很大其他特性：\n 在服务端仅开启一个端口（需要是 UDP 端口）就可用代理多个服务端应用，在客户端可用通过 token 访问指定的应用 服务端可用代理本地套接字应用程序，在服务端可用把本地套接字流量转为 quic 流量，然后在客户端在转为 TCP 或者本地套接字 加密，quic 是强制有 ssl 加密层的，即使应用程序没有配置 ssl 证书也可用通过 quic-tun 放心的在公网上传输  一个典型的应用场景：在服务端有多个虚拟机打开了 VNC 或 SPICE，在正常情况下这要求服务器暴露每个虚机的 VNC/SPICE 端口，而且这些端口还是不固定的，这无疑增加了安全风险，加重了安全策略的复杂度。借助 quic-tun，服务器就只需要向外暴露一个端口即可，而且还可以放心的把这个端口暴露在公网上。\n","description":"","tags":null,"title":"Kungze","uri":"/documents/"},{"content":"负载均衡器是一个很重要的 k8s 所依赖的基础设施组件。市场上存在的很多解决方案都是基于 IaaS 的，由 IaaS 平台提供负载均衡器功能；还有一些基于外部负载均衡器设备的的解决方案，这些方案需要购买厂商的设备。这些方案对于 k8s 开发学习人员，还是私有云 paas 平台的实施管理人员都是过重的，成本过高。metallb 就是为在裸金属上部署的 k8s 平台提供一个轻量的外部负载均衡器的软件。metallb 支持两种模式，BGP 模式 和 layer 2 模式。由于 BGP 模式需要有外部路由器配合，所以我们这里介绍更为简单的 layer 2 模式。\n安装 metallb 的安装相对来说比较简短，执行下面两个命令即可\nkubectl apply -f https://raw.githubusercontent.com/metallb/metallb/main/manifests/namespace.yaml kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/main/manifests/metallb.yaml 配置 layer 2 模式 metallb 部署简单，但是存在单点故障，在生产环境建议使用 BGP 模式。\n如果 kube-proxy 使用的是 ipvs 模式，我们首先需要设置 kube-proxy 的 strictARP 为 true。\nmode:\"ipvs\"ipvs:strictARP:true下面是配置 layer 2 模式的示例文件：\napiVersion:v1kind:ConfigMapmetadata:namespace:metallb-systemname:configdata:config:|address-pools: - name: my-ip-space protocol: layer2 addresses: - 192.168.1.240/28我们把上面内容保存在一个文件中，如 config-layer-2.yaml 中，然后我们执行：\nkubectl apply -f config-layer-2.yaml metallb controller 会自动扫描这个 ConfigMap 加载配置。\n特殊的，如果我们没有连续的整段 IP，我们也可以通过下面这种方式配置零散的 IP\napiVersion:v1kind:ConfigMapmetadata:namespace:metallb-systemname:configdata:config:|address-pools: - name: my-ip-space protocol: layer2 addresses: - 192.168.1.240/32 - 192.168.1.250/32 - 192.168.1.243/32使用示例 使用下面的 manifest，我们创建一个 nginx deployment 和 service，service 类型是 LoadBalancer 的\napiVersion:apps/v1kind:Deploymentmetadata:name:nginxspec:selector:matchLabels:app:nginxtemplate:metadata:labels:app:nginxspec:containers:- name:nginximage:nginx:1ports:- name:httpcontainerPort:80---apiVersion:v1kind:Servicemetadata:name:nginxspec:ports:- name:httpport:80protocol:TCPtargetPort:80selector:app:nginxtype:LoadBalancerkubectl apply 执行成功后，执行下面命令查看创建的 service 的信息\n$ kubectl get svc nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx LoadBalancer 10.68.199.238 192.168.1.243 80:30349/TCP 2h 可以看到 EXTERNAL-IP 为 192.168.1.243，打开浏览器输入 http://192.168.1.243，可以成功访问我们刚刚部署的 nginx:\n","description":"","tags":null,"title":"metallb L2 模式部署","uri":"/documents/post/metallb/"},{"content":"","description":"","tags":null,"title":"Tags","uri":"/documents/tags/"}]